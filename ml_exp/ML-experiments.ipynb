{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284b5c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, Lars\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860b99f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a317881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"s3://ai-diennea/data/export_wonkit_20210630102441.csv.gz\")\n",
    "# convert datetime format\n",
    "data['EVENT.DATE'] = pd.to_datetime(data['EVENT.DATE'], format='%Y/%m/%d %H:%M')\n",
    "# add the day of the week column\n",
    "day_of_week = []\n",
    "for i in range(len(data)):\n",
    "    day_of_week.append(data[\"EVENT.DATE\"][i].day_name())\n",
    "data['day_of_week'] = pd.DataFrame(day_of_week)\n",
    "\n",
    "df = pd.read_csv(\"s3://ai-diennea/data/df.csv\")\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "X = pd.read_csv(\"s3://ai-diennea/data/X.csv\")\n",
    "X = X.drop(['Unnamed: 0'], axis=1)\n",
    "y = pd.read_csv(\"s3://ai-diennea/data/y.csv\")\n",
    "y = y.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86f6a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_open_hour_range = 36\n",
    "open_click_hour_range = 24\n",
    "\n",
    "def exp_decay_fit(x, sent_open_hour_range):\n",
    "    '''Return a value from 0 to 1 following an exponential decreasing function.'''\n",
    "    if x > sent_open_hour_range*60:\n",
    "        return .0\n",
    "    if x < 0:\n",
    "        return 1\n",
    "    return math.exp(15*((-math.log(2)/(sent_open_hour_range*60))*x) + math.log(2))/2\n",
    "\n",
    "def get_all_indexes(hash_mex, hash_contact, data):\n",
    "    '''Given the hash message, hash contact and raw data, return the indexes\n",
    "    for the hash message and hash contact in the raw data.Ã¹'''\n",
    "    return data.index[(data['HashMessaggio'] == hash_mex) & (data['HashContatto'] == hash_contact)]\n",
    "\n",
    "\n",
    "def from_min_to_hour_and_min(mins):\n",
    "    '''Given the minutes, return a string that represents the hour and minutes.'''\n",
    "    hours = int(round(mins)) // 60\n",
    "    minutes = int(round(mins)) % 60\n",
    "    return \"{}:{}\".format(hours, minutes)\n",
    "\n",
    "def evaluate(df, X, data, sent_open_hour_range, preds, never_opened=True):\n",
    "    '''Given the prediction of the model \"preds\" returns:\n",
    "    - how many fitSA are better than the ground truth fitSA\n",
    "    - how many fitSA are equal than the ground truth fitSA\n",
    "    - how many fitSA are worse than the ground truth fitSA.\n",
    "    If never_opened == True, then consider also the sent mails that have never been opened. Not otherwise.\n",
    "    '''\n",
    "    # from [0, 1] to mins\n",
    "    for i in range(len(preds)):\n",
    "        preds[i] *= 24*60 # this 24*60 is requierd to convert mins from [0, 1] into real mins\n",
    "    # get fitSA using the predicted sent\n",
    "    fitSA_preds = []\n",
    "    fitSC_preds = []\n",
    "    for i in range(len(df)):\n",
    "        fitSA_preds.append(compute_fitSA_evaluation(df['HashMessaggio'][i], df['HashContatto'][i], sent_open_hour_range, data, preds[i], never_opened=never_opened))\n",
    "        fitSC_preds.append(compute_fitSC_evaluation(df['HashMessaggio'][i], df['HashContatto'][i], sent_open_hour_range, data, preds[i], never_opened=never_opened))\n",
    "    \n",
    "    total_mex = len(fitSA_preds)\n",
    "    predicted_sent_better_usual_sent = 0\n",
    "    predicted_sent_equal_usual_sent = 0\n",
    "    predicted_sent_worst_usual_sent = 0\n",
    "    if never_opened == True:\n",
    "        for i in range(total_mex):\n",
    "            if fitSA_preds[i] > X.loc[i, 'fitSA']:\n",
    "                predicted_sent_better_usual_sent += 1\n",
    "            elif fitSA_preds[i] == X.loc[i, 'fitSA']:\n",
    "                predicted_sent_equal_usual_sent += 1\n",
    "            elif fitSC_preds[i] > X.loc[i, 'fitAC']:\n",
    "                predicted_sent_better_usual_sent += 1\n",
    "            elif fitSC_preds[i] == X.loc[i, 'fitAC']:\n",
    "                predicted_sent_equal_usual_sent += 1\n",
    "            else:\n",
    "                predicted_sent_worst_usual_sent += 1 # case fitSC predetta < fitAC reale\n",
    "        return predicted_sent_better_usual_sent/total_mex, predicted_sent_equal_usual_sent/total_mex, predicted_sent_worst_usual_sent/total_mex\n",
    "    else:\n",
    "        total_mex_different_from_mins_one = len([i for i in fitSA_preds if i > -1])\n",
    "        for i in range(total_mex):\n",
    "            if fitSA_preds[i] != -1 and fitSA_preds[i] > X.loc[i, 'fitSA']:\n",
    "                predicted_sent_better_usual_sent += 1\n",
    "            elif fitSA_preds[i] != -1 and fitSA_preds[i] == X.loc[i, 'fitSA']:\n",
    "                predicted_sent_equal_usual_sent += 1\n",
    "            elif fitSC_preds[i] != -1 and fitSC_preds[i] > X.loc[i, 'fitAC']:\n",
    "                predicted_sent_better_usual_sent += 1\n",
    "            elif fitSC_preds[i] != -1 and fitSC_preds[i] == X.loc[i, 'fitAC']:\n",
    "                predicted_sent_equal_usual_sent += 1\n",
    "            elif fitSA_preds[i] != -1 or fitSC_preds[i] != -1:\n",
    "                predicted_sent_worst_usual_sent += 1 # case fitSC predetta < fitAC reale    \n",
    "        return predicted_sent_better_usual_sent/total_mex_different_from_mins_one, predicted_sent_equal_usual_sent/total_mex_different_from_mins_one, predicted_sent_worst_usual_sent/total_mex_different_from_mins_one\n",
    "    \n",
    "def compute_fitSA_evaluation(hash_mex, hash_contact, sent_open_hour_range, data, sent_pred, never_opened=True):\n",
    "    '''Given the hash message, hash contact, raw data and the predicted sent hour,\n",
    "    return the fitSA for that contact and message.'''\n",
    "    sent_pred = from_min_to_hour_and_min(sent_pred)\n",
    "    df2 = data[(data['HashMessaggio'] == hash_mex)]\n",
    "    df3 = df2[(df2['HashContatto'] == hash_contact)]\n",
    "    df4 = df3[(df3['EVENT.TYPE'] == 'Open')]\n",
    "    opens = list(df4['EVENT.DATE'])\n",
    "    df5 = df3[(df3['EVENT.TYPE'] == 'Click')]\n",
    "    clicks = list(df5['EVENT.DATE'])\n",
    "    \n",
    "    oldest = None\n",
    "    if opens != []:\n",
    "        for i in opens:\n",
    "            if oldest is None:\n",
    "                oldest = i\n",
    "            elif i < oldest:\n",
    "                oldest = i\n",
    "    elif clicks != []:\n",
    "        for i in clicks:\n",
    "            if oldest is None:\n",
    "                oldest = i\n",
    "            elif i < oldest:\n",
    "                oldest = i\n",
    "    else: # this means that the mail has never been opened/clicked\n",
    "        if never_opened == True:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    oldest = str(oldest.hour) +\":\"+ str(oldest.minute)\n",
    "    oldest = pd.to_datetime(oldest, format='%H:%M')\n",
    "    sent_pred = pd.to_datetime(sent_pred, format='%H:%M')\n",
    "    # compute minutes of the distance between sent-open/sent-click\n",
    "    mins = ((oldest - sent_pred).days*24*60) + ((oldest - sent_pred).seconds//3600)*60 + ((oldest - sent_pred).seconds//60)%60\n",
    "    return exp_decay_fit(mins, sent_open_hour_range)\n",
    "\n",
    "def compute_fitSC_evaluation(hash_mex, hash_contact, open_click_hour_range, data, sent_pred, never_opened=True):\n",
    "    '''Given the message, the hash contact and the data (raw data), return the fitAC\n",
    "    for that hash message and hash contact.'''\n",
    "    sent_pred = from_min_to_hour_and_min(sent_pred)\n",
    "    df2 = data[(data['HashMessaggio'] == hash_mex)]\n",
    "    df3 = df2[(df2['HashContatto'] == hash_contact)]\n",
    "    df4 = df3[(df3['EVENT.TYPE'] == 'Click')]\n",
    "    clicks = list(df4['EVENT.DATE'])\n",
    "    \n",
    "    oldest_click = None\n",
    "\n",
    "    if clicks == []: # covers cases when a message is sent and is never open and never clicked\n",
    "        if never_opened == True:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        for i in clicks: # get oldest click\n",
    "            if oldest_click is None:\n",
    "                oldest_click = i\n",
    "            elif i < oldest_click:\n",
    "                oldest_click = i\n",
    "    # compute minutes of the distance between sent pred-click\n",
    "    oldest_click = str(oldest_click.hour) +\":\"+ str(oldest_click.minute)\n",
    "    #print(oldest_click)\n",
    "    oldest_click = pd.to_datetime(oldest_click, format='%H:%M')\n",
    "    sent_pred = pd.to_datetime(sent_pred, format='%H:%M')\n",
    "    # compute minutes of the distance between sent-open/sent-click\n",
    "    mins = ((oldest_click - sent_pred).days*24*60) + ((oldest_click - sent_pred).seconds//3600)*60 + ((oldest_click - sent_pred).seconds//60)%60\n",
    "    return exp_decay_fit(mins, sent_open_hour_range)\n",
    "\n",
    "def split_train_test_by_lifetime(X, df, data, test_size, random_seed):\n",
    "    d7 = {}\n",
    "    for i in data[\"HashContatto\"].unique():\n",
    "        if i not in d7:\n",
    "            d7[i] = []\n",
    "    for i in range(len(data)):\n",
    "        if \"nan\" not in str(data[\"EVENT.DATE\"][i]) and (str(data[\"EVENT.TYPE\"][i]) == 'Open' or str(data[\"EVENT.TYPE\"][i]) == 'Click'):\n",
    "            d7[data[\"HashContatto\"][i]].append(data[\"EVENT.DATE\"][i])\n",
    "    # Here I merge in the same category (assign 0 days) who never opened with the users that opened just once\n",
    "    for i in data[\"HashContatto\"].unique():\n",
    "        if len(d7[i]) == 0 or len(d7[i]) == 1:\n",
    "            d7[i] = 0 # 0 days as lifetime\n",
    "        else:\n",
    "        # retain newest and oldest date\n",
    "            newest_date = d7[i][0] # get the first date\n",
    "            oldest_date = d7[i][0] # get the first date\n",
    "            for j in d7[i]:\n",
    "                if j > newest_date:\n",
    "                    newest_date = j\n",
    "                if j < oldest_date:\n",
    "                    oldest_date = j\n",
    "            # assign the lifetitme for the contact i\n",
    "            d7[i] = (newest_date - oldest_date).days\n",
    "    df['Lifetime'] = 0\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, 'Lifetime'] = d7[df['HashContatto'][i]]\n",
    "    lt = df['Lifetime'].to_numpy()\n",
    "    zero = []\n",
    "    today = []\n",
    "    between = []\n",
    "    for i in range(len(lt)):\n",
    "        if lt[i] == 0:\n",
    "            zero.append(i)\n",
    "        elif lt[i] >= 320: # TODO: here I assume that a user still open today whether his lifetime is greater than or equal than 320 (it means that the last time he opened is 1 month ago)\n",
    "            today.append(i)\n",
    "        else:\n",
    "            between.append(i)\n",
    "            \n",
    "    random.Random(random_seed).shuffle(zero) # 39%\n",
    "    random.Random(random_seed).shuffle(today) # 17%\n",
    "    random.Random(random_seed).shuffle(between) # 43%\n",
    "    \n",
    "    percentage_zero_train = round(len(zero) - (test_size * len(zero)))\n",
    "    percentage_today_train = round(len(today) - (test_size * len(today)))\n",
    "    percentage_between_train = round(len(between) - (test_size * len(between)))\n",
    "    \n",
    "    train_indexes_zero = zero[:percentage_zero_train]\n",
    "    test_indexes_zero = zero[percentage_zero_train:]\n",
    "\n",
    "    train_indexes_today = today[:percentage_today_train]\n",
    "    test_indexes_today = today[percentage_today_train:]\n",
    "\n",
    "    train_indexes_between = between[:percentage_between_train]\n",
    "    test_indexes_between = between[percentage_between_train:]\n",
    "    \n",
    "    X_train = X.iloc[train_indexes_zero + train_indexes_today + train_indexes_between, :]\n",
    "    y_train = df.iloc[train_indexes_zero + train_indexes_today + train_indexes_between, :]['Label']\n",
    "    X_test = X.iloc[test_indexes_zero + test_indexes_today + test_indexes_between, :]\n",
    "    y_test = df.iloc[test_indexes_zero + test_indexes_today + test_indexes_between, :]['Label']\n",
    "    \n",
    "    df_train = df.iloc[train_indexes_zero + train_indexes_today + train_indexes_between, :]\n",
    "    df_test = df.iloc[test_indexes_zero + test_indexes_today + test_indexes_between, :]\n",
    "    return X_train, X_test, y_train, y_test, df_train, df_test    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe5c4e",
   "metadata": {},
   "source": [
    "## Algoritmi di learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc2218",
   "metadata": {},
   "source": [
    "### Random Forest regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433aa419",
   "metadata": {},
   "source": [
    "Suddivisione **80/20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919a3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "df_train, df_test = train_test_split(df, test_size=0.34, random_state=42)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "# test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=42)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 296.559374332428 seconds ---\n",
      "(0.07084616778401884, 0.9222685269070484, 0.006885305308932777)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42, n_estimators=200, max_features='auto', bootstrap=True, min_samples_split=2, min_samples_leaf=2, oob_score=True, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "X_test_fit = X_test.copy()\n",
    "X_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "X_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "preds = rf.predict(X_test_fit)\n",
    "\n",
    "start_time = time.time()\n",
    "out = evaluate(df_test, X_test, data, sent_open_hour_range, preds)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6adfca7",
   "metadata": {},
   "source": [
    "### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6a546c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07186416389534987, 0.9212821022681279, 0.006853733836522267)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = X.copy()\n",
    "g = y.copy()\n",
    "res = []\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "for train_index, test_index in kf.split(M):\n",
    "    M_train, M_test = M.iloc[train_index], M.iloc[test_index]\n",
    "    g_train, g_test = g.iloc[train_index], g.iloc[test_index]\n",
    "    \n",
    "    rf = RandomForestRegressor(n_estimators=200, max_features='auto', bootstrap=True, min_samples_split=2, min_samples_leaf=2, oob_score=True, n_jobs=-1)\n",
    "    rf.fit(M_train, g_train)\n",
    "\n",
    "    M_test_fit = M_test.copy()\n",
    "    M_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "    M_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "    preds = rf.predict(M_test_fit)\n",
    "    my_df = df.iloc[test_index]\n",
    "    my_df.reset_index(drop=True, inplace=True)\n",
    "    M_test.reset_index(drop=True, inplace=True)\n",
    "    res.append(evaluate(my_df, M_test, data, sent_open_hour_range, preds))\n",
    "better = 0\n",
    "equal = 0\n",
    "worst = 0\n",
    "for i in range(5):\n",
    "    better += res[i][0]\n",
    "    equal += res[i][1]\n",
    "    worst += res[i][2]\n",
    "better/5, equal/5, worst/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b97c7",
   "metadata": {},
   "source": [
    "### Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dfc88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.07169259202217773, 0.9225319574926845, 0.005775450485137841)\n",
      "(0.07289901945685097, 0.9212485240515427, 0.005852456491606346)\n",
      "(0.07293318233295583, 0.9208607021517554, 0.006206115515288788)\n"
     ]
    }
   ],
   "source": [
    "for i in [0.2, 0.3, 0.34]:\n",
    "    X_train, X_test, y_train, y_test, df_train, df_test = split_train_test_by_lifetime(X, df, data, test_size=i, random_seed=42)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=42, n_estimators=200, max_features='auto', bootstrap=True, min_samples_split=2, min_samples_leaf=2, oob_score=True,n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    X_test_fit = X_test.copy()\n",
    "    X_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "    X_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "    preds = rf.predict(X_test_fit)\n",
    "\n",
    "    print(evaluate(df_test, X_test, data, sent_open_hour_range, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19491519",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2d55bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "df_train, df_test = train_test_split(df, test_size=0.30, random_state=42)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "# test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f1ff6620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "92f99460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07505518763796909, 0.920889162688023, 0.004055649674007906)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_fit = X_test.copy()\n",
    "X_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "X_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "preds = model.predict(X_test_fit)\n",
    "\n",
    "p = []\n",
    "for i in range(len(preds)):\n",
    "    p.append(preds[i][0])\n",
    "evaluate(df_test, X_test, data, sent_open_hour_range, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67006461",
   "metadata": {},
   "source": [
    "### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1e5e05b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09230224356421632, 0.9024457914186309, 0.005251965017152739)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = X.copy()\n",
    "g = y.copy()\n",
    "res = []\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "for train_index, test_index in kf.split(M):\n",
    "    M_train, M_test = M.iloc[train_index], M.iloc[test_index]\n",
    "    g_train, g_test = g.iloc[train_index], g.iloc[test_index]\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(M_train, g_train)\n",
    "\n",
    "    M_test_fit = M_test.copy()\n",
    "    M_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "    M_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "    preds = model.predict(M_test_fit)\n",
    "    my_df = df.iloc[test_index]\n",
    "    my_df.reset_index(drop=True, inplace=True)\n",
    "    M_test.reset_index(drop=True, inplace=True)\n",
    "    p = []\n",
    "    for i in range(len(preds)):\n",
    "        p.append(preds[i][0])\n",
    "    res.append(evaluate(my_df, M_test, data, sent_open_hour_range, p))\n",
    "better = 0\n",
    "equal = 0\n",
    "worst = 0\n",
    "for i in range(5):\n",
    "    better += res[i][0]\n",
    "    equal += res[i][1]\n",
    "    worst += res[i][2]\n",
    "better/5, equal/5, worst/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649d78b",
   "metadata": {},
   "source": [
    "### Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6137d865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0950254119821346, 0.9022793777914677, 0.002695210226397659)\n",
      "(0.08326916166127625, 0.9124185019764876, 0.004312336362236255)\n",
      "(0.07941109852774632, 0.9147904869762175, 0.00579841449603624)\n"
     ]
    }
   ],
   "source": [
    "for i in [0.2, 0.3, 0.34]:    \n",
    "    X_train, X_test, y_train, y_test, df_train, df_test = split_train_test_by_lifetime(X, df, data, test_size=i, random_seed=42)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    X_test_fit = X_test.copy()\n",
    "    X_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "    X_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "    preds = model.predict(X_test_fit)\n",
    "    print(evaluate(df_test, X_test, data, sent_open_hour_range, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e82094",
   "metadata": {},
   "source": [
    "## Bayesian regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08ddba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.13988041311831853, 0.8574922993295887, 0.0026272875520927702)\n"
     ]
    }
   ],
   "source": [
    "# train set\n",
    "df_train, df_test = train_test_split(df, test_size=0.34, random_state=42)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "# test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=42)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "model = BayesianRidge(tol=0.001, lambda_1=1e-9, lambda_2=0.001, alpha_1=0.001, alpha_2=1e-9)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test_fit = X_test.copy()\n",
    "X_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "X_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "preds = model.predict(X_test_fit)\n",
    "\n",
    "print(evaluate(df_test, X_test, data, sent_open_hour_range, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c17e0d",
   "metadata": {},
   "source": [
    "### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fbf475c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09636883558319168, 0.898671863736908, 0.004959300679900472)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = X.copy()\n",
    "g = y.copy()\n",
    "res = []\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "for train_index, test_index in kf.split(M):\n",
    "    M_train, M_test = M.iloc[train_index], M.iloc[test_index]\n",
    "    g_train, g_test = g.iloc[train_index], g.iloc[test_index]\n",
    "    \n",
    "    model = BayesianRidge(tol=0.001, lambda_1=1e-9, lambda_2=0.001, alpha_1=0.001, alpha_2=1e-9)\n",
    "    model.fit(M_train, g_train)\n",
    "\n",
    "    M_test_fit = M_test.copy()\n",
    "    M_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "    M_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "    preds = model.predict(M_test_fit)\n",
    "    my_df = df.iloc[test_index]\n",
    "    my_df.reset_index(drop=True, inplace=True)\n",
    "    M_test.reset_index(drop=True, inplace=True)\n",
    "    res.append(evaluate(my_df, M_test, data, sent_open_hour_range, preds))\n",
    "better = 0\n",
    "equal = 0\n",
    "worst = 0\n",
    "for i in range(5):\n",
    "    better += res[i][0]\n",
    "    equal += res[i][1]\n",
    "    worst += res[i][2]\n",
    "better/5, equal/5, worst/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e5047",
   "metadata": {},
   "source": [
    "### Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf9da98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.09641152009856768, 0.9008932696750347, 0.002695210226397659)\n",
      "(0.08403922172596129, 0.9116484419118025, 0.004312336362236255)\n",
      "(0.07959229898074745, 0.9147451868629671, 0.0056625141562853904)\n"
     ]
    }
   ],
   "source": [
    "for i in [0.2, 0.3, 0.34]:\n",
    "    X_train, X_test, y_train, y_test, df_train, df_test = split_train_test_by_lifetime(X, df, data, test_size=i, random_seed=42)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    model = BayesianRidge(tol=0.001, lambda_1=1e-9, lambda_2=0.001, alpha_1=0.001, alpha_2=1e-9)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    X_test_fit = X_test.copy()\n",
    "    X_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "    X_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "    preds = model.predict(X_test_fit)\n",
    "\n",
    "    print(evaluate(df_test, X_test, data, sent_open_hour_range, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec593af3",
   "metadata": {},
   "source": [
    "## Least Angle Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdc7b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "# test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2cf1489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lars(n_nonzero_coefs=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Lars(n_nonzero_coefs=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25bd8e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1558601570922532, 0.8409055906360696, 0.0032342522716771907)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_fit = X_test.copy()\n",
    "X_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "X_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "preds = model.predict(X_test_fit)\n",
    "\n",
    "evaluate(df_test, X_test, data, sent_open_hour_range, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c0168",
   "metadata": {},
   "source": [
    "**never_opened=False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28e784bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6644780039395929, 0.07550886408404466, 0.26001313197636244)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_fit = X_test.copy()\n",
    "X_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "X_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "preds = model.predict(X_test_fit)\n",
    "\n",
    "evaluate(df_test, X_test, data, sent_open_hour_range, preds, never_opened=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4e721b",
   "metadata": {},
   "source": [
    "### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "865f05c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16020828923847436, 0.8366959578103156, 0.003095752951209979)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = X.copy()\n",
    "g = y.copy()\n",
    "res = []\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "for train_index, test_index in kf.split(M):\n",
    "    M_train, M_test = M.iloc[train_index], M.iloc[test_index]\n",
    "    g_train, g_test = g.iloc[train_index], g.iloc[test_index]\n",
    "    \n",
    "    model = Lars(n_nonzero_coefs=1)\n",
    "    model.fit(M_train, g_train)\n",
    "\n",
    "    M_test_fit = M_test.copy()\n",
    "    M_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "    M_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "    preds = model.predict(M_test_fit)\n",
    "    my_df = df.iloc[test_index]\n",
    "    my_df.reset_index(drop=True, inplace=True)\n",
    "    M_test.reset_index(drop=True, inplace=True)\n",
    "    res.append(evaluate(my_df, M_test, data, sent_open_hour_range, preds))\n",
    "better = 0\n",
    "equal = 0\n",
    "worst = 0\n",
    "for i in range(5):\n",
    "    better += res[i][0]\n",
    "    equal += res[i][1]\n",
    "    worst += res[i][2]\n",
    "better/5, equal/5, worst/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb3145",
   "metadata": {},
   "source": [
    "### Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7bbde441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.15847836131218235, 0.8385954104420145, 0.002926228245803173)\n",
      "(0.1609425535191745, 0.836182555572668, 0.002874890908157503)\n",
      "(0.16117780294450737, 0.835832389580974, 0.002989807474518686)\n"
     ]
    }
   ],
   "source": [
    "for i in [0.20, 0.30, 0.34]:\n",
    "    X_train, X_test, y_train, y_test, df_train, df_test = split_train_test_by_lifetime(X, df, data, test_size=i, random_seed=42)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    model = model = Lars(n_nonzero_coefs=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    X_test_fit = X_test.copy()\n",
    "    X_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "    X_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "    preds = model.predict(X_test_fit)\n",
    "\n",
    "    print(evaluate(df_test, X_test, data, sent_open_hour_range, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d888de8",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5a4379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               12672     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 177,537\n",
      "Trainable params: 177,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cb5be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41553 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41553/41553 [==============================] - 7s 174us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
      "Epoch 2/10\n",
      "41553/41553 [==============================] - 10s 237us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
      "Epoch 3/10\n",
      "41553/41553 [==============================] - 7s 170us/step - loss: 0.0067 - mean_absolute_error: 0.0067 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
      "Epoch 4/10\n",
      "41553/41553 [==============================] - 7s 168us/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
      "Epoch 5/10\n",
      "41553/41553 [==============================] - 7s 167us/step - loss: 0.0061 - mean_absolute_error: 0.0061 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
      "Epoch 6/10\n",
      "41553/41553 [==============================] - 7s 168us/step - loss: 0.0058 - mean_absolute_error: 0.0058 - val_loss: 0.0052 - val_mean_absolute_error: 0.0052\n",
      "Epoch 7/10\n",
      "41553/41553 [==============================] - 7s 168us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0047 - val_mean_absolute_error: 0.0047\n",
      "Epoch 8/10\n",
      "41553/41553 [==============================] - 7s 168us/step - loss: 0.0055 - mean_absolute_error: 0.0055 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
      "Epoch 9/10\n",
      "41553/41553 [==============================] - 7s 169us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
      "Epoch 10/10\n",
      "41553/41553 [==============================] - 7s 169us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "(0.1591714153703989, 0.8376713383643924, 0.0031572462652086864)\n",
      "Train on 36359 samples, validate on 9090 samples\n",
      "Epoch 1/10\n",
      "36359/36359 [==============================] - 6s 170us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045\n",
      "Epoch 2/10\n",
      "36359/36359 [==============================] - 7s 180us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
      "Epoch 3/10\n",
      "36359/36359 [==============================] - 6s 173us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
      "Epoch 4/10\n",
      "36359/36359 [==============================] - 6s 172us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
      "Epoch 5/10\n",
      "36359/36359 [==============================] - 6s 172us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0047 - val_mean_absolute_error: 0.0047\n",
      "Epoch 6/10\n",
      "36359/36359 [==============================] - 6s 174us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0048 - val_mean_absolute_error: 0.0048\n",
      "Epoch 7/10\n",
      "36359/36359 [==============================] - 6s 173us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0048 - val_mean_absolute_error: 0.0048\n",
      "Epoch 8/10\n",
      "36359/36359 [==============================] - 6s 168us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
      "Epoch 9/10\n",
      "36359/36359 [==============================] - 6s 170us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "Epoch 10/10\n",
      "36359/36359 [==============================] - 6s 170us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0047 - val_mean_absolute_error: 0.0047\n",
      "(0.10544689152420555, 0.8884953026336054, 0.006057805842189024)\n",
      "Train on 34281 samples, validate on 8571 samples\n",
      "Epoch 1/10\n",
      "34281/34281 [==============================] - 6s 168us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0054 - val_mean_absolute_error: 0.0054\n",
      "Epoch 2/10\n",
      "34281/34281 [==============================] - 6s 167us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045\n",
      "Epoch 3/10\n",
      "34281/34281 [==============================] - 6s 168us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0048 - val_mean_absolute_error: 0.0048\n",
      "Epoch 4/10\n",
      "34281/34281 [==============================] - 6s 167us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0052 - val_mean_absolute_error: 0.0052\n",
      "Epoch 5/10\n",
      "34281/34281 [==============================] - 6s 169us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 6/10\n",
      "34281/34281 [==============================] - 6s 172us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
      "Epoch 7/10\n",
      "34281/34281 [==============================] - 6s 172us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
      "Epoch 8/10\n",
      "34281/34281 [==============================] - 6s 172us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0057 - val_mean_absolute_error: 0.0057\n",
      "Epoch 9/10\n",
      "34281/34281 [==============================] - 6s 171us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
      "Epoch 10/10\n",
      "34281/34281 [==============================] - 6s 170us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0046 - val_mean_absolute_error: 0.0046\n",
      "(0.09399347707918101, 0.9000724768979887, 0.005934046022830223)\n"
     ]
    }
   ],
   "source": [
    "for i in [0.2, 0.3, 0.34]:\n",
    "    # train set\n",
    "    df_train, df_test = train_test_split(df, test_size=i, random_state=42)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    # test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=i, random_state=42)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    NN_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split = 0.2)\n",
    "\n",
    "    X_test_fit = X_test.copy()\n",
    "    X_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "    X_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "    preds = NN_model.predict(X_test_fit)\n",
    "\n",
    "    p = []\n",
    "    for i in range(len(preds)):\n",
    "        p.append(preds[i][0])\n",
    "    print(evaluate(df_test, X_test, data, sent_open_hour_range, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f2f2ad",
   "metadata": {},
   "source": [
    "### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b2217e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41553 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41553/41553 [==============================] - 7s 173us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
      "Epoch 2/10\n",
      "41553/41553 [==============================] - 7s 168us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 3/10\n",
      "41553/41553 [==============================] - 7s 173us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "Epoch 4/10\n",
      "41553/41553 [==============================] - 7s 167us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
      "Epoch 5/10\n",
      "41553/41553 [==============================] - 7s 171us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "Epoch 6/10\n",
      "41553/41553 [==============================] - 7s 172us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
      "Epoch 7/10\n",
      "41553/41553 [==============================] - 7s 174us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 8/10\n",
      "41553/41553 [==============================] - 7s 172us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0052 - val_mean_absolute_error: 0.0052\n",
      "Epoch 9/10\n",
      "41553/41553 [==============================] - 7s 171us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
      "Epoch 10/10\n",
      "41553/41553 [==============================] - 7s 167us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
      "Train on 41553 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41553/41553 [==============================] - 7s 168us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 2/10\n",
      "41553/41553 [==============================] - 7s 168us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
      "Epoch 3/10\n",
      "41553/41553 [==============================] - 7s 169us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0052 - val_mean_absolute_error: 0.0052\n",
      "Epoch 4/10\n",
      "41553/41553 [==============================] - 7s 167us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0052 - val_mean_absolute_error: 0.0052\n",
      "Epoch 5/10\n",
      "41553/41553 [==============================] - 7s 168us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
      "Epoch 6/10\n",
      "41553/41553 [==============================] - 8s 195us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 7/10\n",
      "41553/41553 [==============================] - 7s 174us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
      "Epoch 8/10\n",
      "41553/41553 [==============================] - 7s 173us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
      "Epoch 9/10\n",
      "41553/41553 [==============================] - 7s 177us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
      "Epoch 10/10\n",
      "41553/41553 [==============================] - 7s 173us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Train on 41553 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41553/41553 [==============================] - 7s 169us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
      "Epoch 2/10\n",
      "41553/41553 [==============================] - 7s 166us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "Epoch 3/10\n",
      "41553/41553 [==============================] - 7s 171us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0054 - val_mean_absolute_error: 0.0054\n",
      "Epoch 4/10\n",
      "41553/41553 [==============================] - 9s 226us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 5/10\n",
      "41553/41553 [==============================] - 7s 176us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 6/10\n",
      "41553/41553 [==============================] - 7s 167us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
      "Epoch 7/10\n",
      "41553/41553 [==============================] - 7s 170us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0052 - val_mean_absolute_error: 0.0052\n",
      "Epoch 8/10\n",
      "41553/41553 [==============================] - 7s 170us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 9/10\n",
      "41553/41553 [==============================] - 7s 167us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
      "Epoch 10/10\n",
      "41553/41553 [==============================] - 7s 172us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
      "Train on 41554 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41554/41554 [==============================] - 7s 171us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
      "Epoch 2/10\n",
      "41554/41554 [==============================] - 7s 172us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0056 - val_mean_absolute_error: 0.0056\n",
      "Epoch 3/10\n",
      "41554/41554 [==============================] - 7s 175us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 4/10\n",
      "41554/41554 [==============================] - 7s 174us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 5/10\n",
      "41554/41554 [==============================] - 7s 171us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 6/10\n",
      "41554/41554 [==============================] - 7s 174us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0048 - val_mean_absolute_error: 0.0048\n",
      "Epoch 7/10\n",
      "41554/41554 [==============================] - 7s 170us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
      "Epoch 8/10\n",
      "41554/41554 [==============================] - 7s 171us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0052 - val_mean_absolute_error: 0.0052\n",
      "Epoch 9/10\n",
      "41554/41554 [==============================] - 7s 173us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0054 - val_mean_absolute_error: 0.0054\n",
      "Epoch 10/10\n",
      "41554/41554 [==============================] - 7s 170us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
      "Train on 41554 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41554/41554 [==============================] - 7s 170us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
      "Epoch 2/10\n",
      "41554/41554 [==============================] - 7s 172us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0052 - val_mean_absolute_error: 0.0052\n",
      "Epoch 3/10\n",
      "41554/41554 [==============================] - 8s 185us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0048 - val_mean_absolute_error: 0.0048\n",
      "Epoch 4/10\n",
      "41554/41554 [==============================] - 7s 172us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "Epoch 5/10\n",
      "41554/41554 [==============================] - 7s 171us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "Epoch 6/10\n",
      "41554/41554 [==============================] - 7s 172us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
      "Epoch 7/10\n",
      "41554/41554 [==============================] - 7s 168us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "Epoch 8/10\n",
      "41554/41554 [==============================] - 7s 171us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "Epoch 9/10\n",
      "41554/41554 [==============================] - 7s 170us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0048 - val_mean_absolute_error: 0.0048\n",
      "Epoch 10/10\n",
      "41554/41554 [==============================] - 7s 177us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0854323541818472, 0.9075751873066583, 0.006992458511494354)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = X.copy()\n",
    "g = y.copy()\n",
    "res = []\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "for train_index, test_index in kf.split(M):\n",
    "    M_train, M_test = M.iloc[train_index], M.iloc[test_index]\n",
    "    g_train, g_test = g.iloc[train_index], g.iloc[test_index]\n",
    "    \n",
    "    model = NN_model\n",
    "    model.fit(M_train, g_train, epochs=10, batch_size=32, validation_split = 0.2)\n",
    "\n",
    "    M_test_fit = M_test.copy()\n",
    "    M_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "    M_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "    \n",
    "    preds = model.predict(M_test_fit)\n",
    "    my_df = df.iloc[test_index]\n",
    "    my_df.reset_index(drop=True, inplace=True)\n",
    "    M_test.reset_index(drop=True, inplace=True)\n",
    "    p = []\n",
    "    for i in range(len(preds)):\n",
    "        p.append(preds[i][0])\n",
    "    res.append(evaluate(my_df, M_test, data, sent_open_hour_range, p))\n",
    "better = 0\n",
    "equal = 0\n",
    "worst = 0\n",
    "for i in range(5):\n",
    "    better += res[i][0]\n",
    "    equal += res[i][1]\n",
    "    worst += res[i][2]\n",
    "better/5, equal/5, worst/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f28ef",
   "metadata": {},
   "source": [
    "### Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c47a9675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41553 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41553/41553 [==============================] - 7s 172us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
      "Epoch 2/10\n",
      "41553/41553 [==============================] - 7s 174us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043\n",
      "Epoch 3/10\n",
      "41553/41553 [==============================] - 7s 167us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0046 - val_mean_absolute_error: 0.0046\n",
      "Epoch 4/10\n",
      "41553/41553 [==============================] - 7s 170us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0047 - val_mean_absolute_error: 0.0047\n",
      "Epoch 5/10\n",
      "41553/41553 [==============================] - 7s 168us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042\n",
      "Epoch 6/10\n",
      "41553/41553 [==============================] - 7s 167us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0048 - val_mean_absolute_error: 0.0048\n",
      "Epoch 7/10\n",
      "41553/41553 [==============================] - 7s 169us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045\n",
      "Epoch 8/10\n",
      "41553/41553 [==============================] - 7s 171us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 9/10\n",
      "41553/41553 [==============================] - 7s 167us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043\n",
      "Epoch 10/10\n",
      "41553/41553 [==============================] - 7s 169us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042\n",
      "(0.09741259818265825, 0.8979670414292314, 0.004620360388110273)\n",
      "Train on 36359 samples, validate on 9090 samples\n",
      "Epoch 1/10\n",
      "36359/36359 [==============================] - 6s 176us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045\n",
      "Epoch 2/10\n",
      "36359/36359 [==============================] - 8s 224us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044\n",
      "Epoch 3/10\n",
      "36359/36359 [==============================] - 6s 170us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045\n",
      "Epoch 4/10\n",
      "36359/36359 [==============================] - 6s 166us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
      "Epoch 5/10\n",
      "36359/36359 [==============================] - 6s 166us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042\n",
      "Epoch 6/10\n",
      "36359/36359 [==============================] - 6s 166us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045\n",
      "Epoch 7/10\n",
      "36359/36359 [==============================] - 6s 167us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045\n",
      "Epoch 8/10\n",
      "36359/36359 [==============================] - 6s 168us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0048 - val_mean_absolute_error: 0.0048\n",
      "Epoch 9/10\n",
      "36359/36359 [==============================] - 6s 170us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
      "Epoch 10/10\n",
      "36359/36359 [==============================] - 6s 168us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045\n",
      "(0.15817033728630833, 0.8397761692078649, 0.002053493505826788)\n",
      "Train on 34282 samples, validate on 8571 samples\n",
      "Epoch 1/10\n",
      "34282/34282 [==============================] - 6s 169us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043\n",
      "Epoch 2/10\n",
      "34282/34282 [==============================] - 7s 197us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
      "Epoch 3/10\n",
      "34282/34282 [==============================] - 7s 211us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0046 - val_mean_absolute_error: 0.0046\n",
      "Epoch 4/10\n",
      "34282/34282 [==============================] - 6s 175us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0048 - val_mean_absolute_error: 0.0048\n",
      "Epoch 5/10\n",
      "34282/34282 [==============================] - 6s 171us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043\n",
      "Epoch 6/10\n",
      "34282/34282 [==============================] - 6s 171us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042\n",
      "Epoch 7/10\n",
      "34282/34282 [==============================] - 6s 171us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045\n",
      "Epoch 8/10\n",
      "34282/34282 [==============================] - 6s 170us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "Epoch 9/10\n",
      "34282/34282 [==============================] - 6s 167us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044\n",
      "Epoch 10/10\n",
      "34282/34282 [==============================] - 6s 167us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044\n",
      "(0.07103057757644395, 0.9213590033975085, 0.007610419026047565)\n"
     ]
    }
   ],
   "source": [
    "for i in [0.2, 0.3, 0.34]:\n",
    "    X_train, X_test, y_train, y_test, df_train, df_test = split_train_test_by_lifetime(X, df, data, test_size=i, random_seed=42)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    model = NN_model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split = 0.2)\n",
    "\n",
    "\n",
    "    X_test_fit = X_test.copy()\n",
    "    X_test_fit.loc[:, 'fitSA'] = 1 # fitSA\n",
    "    X_test_fit.loc[:, 'fitAC'] = 1 # fitAC\n",
    "    preds = NN_model.predict(X_test_fit)\n",
    "\n",
    "    p = []\n",
    "    for i in range(len(preds)):\n",
    "        p.append(preds[i][0])\n",
    "    print(evaluate(df_test, X_test, data, sent_open_hour_range, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8475d7a",
   "metadata": {},
   "source": [
    "## Regression with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b191f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, glob, sys, numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras import losses\n",
    "from keras import backend as K \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import metrics\n",
    "from keras import models, layers, optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44381ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "# test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a69a87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(min_samples_leaf=2, n_estimators=200, n_jobs=-1,\n",
       "                      oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42, n_estimators=200, max_features='auto', bootstrap=True, min_samples_split=2, min_samples_leaf=2, oob_score=True, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26cdfcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e92100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.10222816e-10, 2.08272816e-08, 1.30319865e-07, 2.72612664e-07,\n",
       "       9.29156849e-07, 1.24821780e-06, 2.52331203e-06, 2.86380571e-06,\n",
       "       4.80629471e-06, 5.86005945e-06, 6.24028196e-06, 6.93961063e-06,\n",
       "       7.10098302e-06, 7.26015824e-06, 8.15152144e-06, 9.14754292e-06,\n",
       "       1.36861135e-05, 1.46800102e-05, 1.97310321e-05, 2.09428692e-05,\n",
       "       2.10419481e-05, 2.12223362e-05, 2.48352894e-05, 2.54027020e-05,\n",
       "       2.60013243e-05, 2.63220531e-05, 2.63809668e-05, 2.68271019e-05,\n",
       "       2.73138541e-05, 2.75358324e-05, 2.89053121e-05, 2.97376040e-05,\n",
       "       3.21101786e-05, 3.41045631e-05, 3.52949784e-05, 3.63827260e-05,\n",
       "       3.66880625e-05, 3.69092634e-05, 3.95015576e-05, 4.75049159e-05,\n",
       "       4.87973581e-05, 4.91780192e-05, 5.25719750e-05, 5.41827669e-05,\n",
       "       5.42325175e-05, 5.43680386e-05, 5.85326750e-05, 6.37560049e-05,\n",
       "       6.38048684e-05, 6.40836389e-05, 6.53483900e-05, 8.60669319e-05,\n",
       "       8.93263983e-05, 9.16216550e-05, 1.08662404e-04, 1.11554682e-04,\n",
       "       1.13892517e-04, 1.27370962e-04, 1.54537333e-04, 1.56064345e-04,\n",
       "       1.56238482e-04, 2.01721041e-04, 2.07525943e-04, 2.18542313e-04,\n",
       "       2.19031021e-04, 2.35875872e-04, 2.39109764e-04, 2.89564212e-04,\n",
       "       3.37469028e-04, 3.38740242e-04, 3.40811449e-04, 3.61471862e-04,\n",
       "       3.64846019e-04, 3.69253435e-04, 4.83199922e-04, 5.68991364e-04,\n",
       "       6.02642985e-04, 6.14077498e-04, 6.18483973e-04, 8.75067808e-04,\n",
       "       8.86054833e-04, 9.25644577e-04, 9.28796800e-04, 1.00810604e-03,\n",
       "       1.33316468e-03, 1.78711887e-03, 2.29967735e-03, 3.73559292e-03,\n",
       "       5.28117999e-03, 7.35631773e-03, 7.67350732e-03, 1.03252255e-02,\n",
       "       8.45613763e-02, 8.61845062e-01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e437909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Z.columns[np.where(rf.feature_importances_ > 7.10098302e-06)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e78333",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.drop(Z.columns.difference(Z.columns[np.where(rf.feature_importances_ > 7.10098302e-06)]), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6a0d290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 32)          320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 9, 9, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 2, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 46,497\n",
      "Trainable params: 45,729\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "droprate=0.25\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=(9,9,1), activation=\"relu\")) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(droprate))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(droprate))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(droprate))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1))\n",
    "\n",
    "def rmsle(y_test, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(K.log(y_pred) - K.log(y_test))))\n",
    "\n",
    "model.compile(loss= 'mean_squared_error', optimizer='adam', metrics=[rmsle])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfb0e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "# test set\n",
    "Z_train, Z_test, y_train, y_test = train_test_split(Z, y, test_size=0.3, random_state=42)\n",
    "Z_train.reset_index(drop=True, inplace=True)\n",
    "Z_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc307d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([])\n",
    "for i in range(len(Z_train)):\n",
    "    row = Z_train.iloc[i, :].to_numpy().reshape(-1, 1).reshape(9, 9, 1)\n",
    "    arr = np.append(arr, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed0cfe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = np.array([])\n",
    "for i in range(len(Z_test)):\n",
    "    row = Z_test.iloc[i, :].to_numpy().reshape(-1, 1).reshape(9, 9, 1)\n",
    "    arr2 = np.append(arr2, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03bb2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train_img = arr.reshape(Z_train.shape[0], 9, 9, 1)\n",
    "Z_test_img = arr2.reshape(Z_test.shape[0], 9, 9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8335a2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36359 samples, validate on 9090 samples\n",
      "Epoch 1/10\n",
      "36359/36359 [==============================] - 23s 634us/step - loss: 4.6490e-04 - rmsle: 0.0372 - val_loss: 3.1456e-04 - val_rmsle: 0.0337\n",
      "Epoch 2/10\n",
      "36359/36359 [==============================] - 23s 638us/step - loss: 4.3277e-04 - rmsle: 0.0356 - val_loss: 1.7088e-04 - val_rmsle: 0.0217\n",
      "Epoch 3/10\n",
      "36359/36359 [==============================] - 23s 635us/step - loss: 5.3403e-04 - rmsle: 0.0400 - val_loss: 0.0013 - val_rmsle: 0.0667\n",
      "Epoch 4/10\n",
      "36359/36359 [==============================] - 27s 733us/step - loss: 4.5464e-04 - rmsle: 0.0365 - val_loss: 3.2318e-04 - val_rmsle: 0.0334\n",
      "Epoch 5/10\n",
      "36359/36359 [==============================] - 26s 712us/step - loss: 4.4937e-04 - rmsle: 0.0362 - val_loss: 1.6209e-04 - val_rmsle: 0.0197\n",
      "Epoch 6/10\n",
      "36359/36359 [==============================] - 26s 710us/step - loss: 4.5629e-04 - rmsle: 0.0363 - val_loss: 1.4290e-04 - val_rmsle: 0.0181\n",
      "Epoch 7/10\n",
      "36359/36359 [==============================] - 28s 782us/step - loss: 4.3952e-04 - rmsle: 0.0359 - val_loss: 3.5175e-04 - val_rmsle: 0.0335\n",
      "Epoch 8/10\n",
      "36359/36359 [==============================] - 26s 713us/step - loss: 4.4686e-04 - rmsle: 0.0364 - val_loss: 1.7395e-04 - val_rmsle: 0.0220\n",
      "Epoch 9/10\n",
      "36359/36359 [==============================] - 26s 711us/step - loss: 4.3157e-04 - rmsle: 0.0353 - val_loss: 3.6848e-04 - val_rmsle: 0.0344\n",
      "Epoch 10/10\n",
      "36359/36359 [==============================] - 26s 715us/step - loss: 4.5068e-04 - rmsle: 0.0365 - val_loss: 1.7073e-04 - val_rmsle: 0.0216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f568a395c88>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Z_train_img, y_train, batch_size=32, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f6c6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_test_img_fit = Z_test_img.copy()\n",
    "Z_test_img_fit[:, 8, 7, 0] = 1 # fitSA\n",
    "Z_test_img_fit[:, 8, 8, 0] = 1 # fitAC\n",
    "preds = model.predict(Z_test_img_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb2897e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6296601],\n",
       "       [0.6286154],\n",
       "       [0.4848866],\n",
       "       ...,\n",
       "       [0.6679155],\n",
       "       [0.4859561],\n",
       "       [0.6300905]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "018d3b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1873299450690487, 0.8115406335027465, 0.0011294214282047332)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = []\n",
    "for i in range(len(preds)):\n",
    "    p.append(preds[i][0])\n",
    "evaluate(df_test, X_test, data, sent_open_hour_range, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b91dd",
   "metadata": {},
   "source": [
    "### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5e2bd265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41553 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41553/41553 [==============================] - 33s 783us/step - loss: 0.0692 - rmsle: nan - val_loss: 0.0020 - val_rmsle: 0.0839\n",
      "Epoch 2/10\n",
      "41553/41553 [==============================] - 30s 710us/step - loss: 0.0057 - rmsle: 0.1451 - val_loss: 0.0022 - val_rmsle: 0.0901\n",
      "Epoch 3/10\n",
      "41553/41553 [==============================] - 28s 685us/step - loss: 0.0035 - rmsle: 0.1122 - val_loss: 0.0017 - val_rmsle: 0.0800\n",
      "Epoch 4/10\n",
      "41553/41553 [==============================] - 29s 698us/step - loss: 0.0024 - rmsle: 0.0937 - val_loss: 0.0010 - val_rmsle: 0.0612\n",
      "Epoch 5/10\n",
      "41553/41553 [==============================] - 29s 695us/step - loss: 0.0020 - rmsle: 0.0847 - val_loss: 7.2356e-04 - val_rmsle: 0.0462\n",
      "Epoch 6/10\n",
      "41553/41553 [==============================] - 31s 748us/step - loss: 0.0015 - rmsle: 0.0736 - val_loss: 3.8936e-04 - val_rmsle: 0.0367\n",
      "Epoch 7/10\n",
      "41553/41553 [==============================] - 29s 687us/step - loss: 0.0013 - rmsle: 0.0669 - val_loss: 3.9913e-04 - val_rmsle: 0.0333\n",
      "Epoch 8/10\n",
      "41553/41553 [==============================] - 29s 694us/step - loss: 0.0011 - rmsle: 0.0614 - val_loss: 2.4169e-04 - val_rmsle: 0.0272\n",
      "Epoch 9/10\n",
      "41553/41553 [==============================] - 29s 688us/step - loss: 9.2598e-04 - rmsle: 0.0562 - val_loss: 2.3245e-04 - val_rmsle: 0.0251\n",
      "Epoch 10/10\n",
      "41553/41553 [==============================] - 29s 699us/step - loss: 8.5619e-04 - rmsle: 0.0542 - val_loss: 5.3091e-04 - val_rmsle: 0.0442\n",
      "Train on 41553 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41553/41553 [==============================] - 29s 691us/step - loss: 7.9316e-04 - rmsle: 0.0519 - val_loss: 5.9730e-04 - val_rmsle: 0.0404\n",
      "Epoch 2/10\n",
      "41553/41553 [==============================] - 32s 760us/step - loss: 6.9674e-04 - rmsle: 0.0484 - val_loss: 2.1720e-04 - val_rmsle: 0.0232\n",
      "Epoch 3/10\n",
      "41553/41553 [==============================] - 29s 687us/step - loss: 6.3011e-04 - rmsle: 0.0458 - val_loss: 2.6509e-04 - val_rmsle: 0.0291\n",
      "Epoch 4/10\n",
      "41553/41553 [==============================] - 29s 697us/step - loss: 6.6110e-04 - rmsle: 0.0476 - val_loss: 2.9581e-04 - val_rmsle: 0.0330\n",
      "Epoch 5/10\n",
      "41553/41553 [==============================] - 29s 700us/step - loss: 6.0867e-04 - rmsle: 0.0452 - val_loss: 2.7348e-04 - val_rmsle: 0.0298\n",
      "Epoch 6/10\n",
      "41553/41553 [==============================] - 29s 693us/step - loss: 5.5553e-04 - rmsle: 0.0430 - val_loss: 2.1889e-04 - val_rmsle: 0.0252\n",
      "Epoch 7/10\n",
      "41553/41553 [==============================] - 29s 700us/step - loss: 5.3711e-04 - rmsle: 0.0414 - val_loss: 1.8560e-04 - val_rmsle: 0.0213\n",
      "Epoch 8/10\n",
      "41553/41553 [==============================] - 31s 749us/step - loss: 5.2146e-04 - rmsle: 0.0408 - val_loss: 5.0950e-04 - val_rmsle: 0.0471\n",
      "Epoch 9/10\n",
      "41553/41553 [==============================] - 29s 696us/step - loss: 5.2152e-04 - rmsle: 0.0406 - val_loss: 2.2757e-04 - val_rmsle: 0.0252\n",
      "Epoch 10/10\n",
      "41553/41553 [==============================] - 29s 703us/step - loss: 4.7057e-04 - rmsle: 0.0381 - val_loss: 1.7687e-04 - val_rmsle: 0.0206\n",
      "Train on 41553 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41553/41553 [==============================] - 31s 756us/step - loss: 5.3033e-04 - rmsle: 0.0410 - val_loss: 4.0237e-04 - val_rmsle: 0.0386\n",
      "Epoch 2/10\n",
      "41553/41553 [==============================] - 31s 741us/step - loss: 5.0030e-04 - rmsle: 0.0386 - val_loss: 2.1236e-04 - val_rmsle: 0.0236\n",
      "Epoch 3/10\n",
      "41553/41553 [==============================] - 29s 689us/step - loss: 4.7700e-04 - rmsle: 0.0380 - val_loss: 1.8054e-04 - val_rmsle: 0.0214\n",
      "Epoch 4/10\n",
      "41553/41553 [==============================] - 32s 761us/step - loss: 4.6367e-04 - rmsle: 0.0373 - val_loss: 2.8761e-04 - val_rmsle: 0.0286\n",
      "Epoch 5/10\n",
      "41553/41553 [==============================] - 29s 694us/step - loss: 4.7883e-04 - rmsle: 0.0379 - val_loss: 2.8767e-04 - val_rmsle: 0.0315\n",
      "Epoch 6/10\n",
      "41553/41553 [==============================] - 29s 688us/step - loss: 4.4722e-04 - rmsle: 0.0365 - val_loss: 1.6739e-04 - val_rmsle: 0.0200\n",
      "Epoch 7/10\n",
      "41553/41553 [==============================] - 28s 679us/step - loss: 4.6251e-04 - rmsle: 0.0372 - val_loss: 7.0322e-04 - val_rmsle: 0.0572\n",
      "Epoch 8/10\n",
      "41553/41553 [==============================] - 29s 694us/step - loss: 4.5221e-04 - rmsle: 0.0366 - val_loss: 3.4460e-04 - val_rmsle: 0.0361\n",
      "Epoch 9/10\n",
      "41553/41553 [==============================] - 29s 698us/step - loss: 4.6427e-04 - rmsle: 0.0371 - val_loss: 2.7960e-04 - val_rmsle: 0.0292\n",
      "Epoch 10/10\n",
      "41553/41553 [==============================] - 31s 758us/step - loss: 4.8531e-04 - rmsle: 0.0380 - val_loss: 2.7264e-04 - val_rmsle: 0.0305\n",
      "Train on 41554 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41554/41554 [==============================] - 31s 757us/step - loss: 4.5009e-04 - rmsle: 0.0363 - val_loss: 1.8046e-04 - val_rmsle: 0.0217\n",
      "Epoch 2/10\n",
      "41554/41554 [==============================] - 31s 754us/step - loss: 5.4182e-04 - rmsle: 0.0397 - val_loss: 1.9367e-04 - val_rmsle: 0.0213\n",
      "Epoch 3/10\n",
      "41554/41554 [==============================] - 31s 753us/step - loss: 4.6420e-04 - rmsle: 0.0367 - val_loss: 2.8967e-04 - val_rmsle: 0.0305\n",
      "Epoch 4/10\n",
      "41554/41554 [==============================] - 32s 758us/step - loss: 4.5289e-04 - rmsle: 0.0364 - val_loss: 2.0015e-04 - val_rmsle: 0.0236\n",
      "Epoch 5/10\n",
      "41554/41554 [==============================] - 32s 758us/step - loss: 4.2523e-04 - rmsle: 0.0349 - val_loss: 2.2894e-04 - val_rmsle: 0.0263\n",
      "Epoch 6/10\n",
      "41554/41554 [==============================] - 34s 826us/step - loss: 4.4095e-04 - rmsle: 0.0357 - val_loss: 2.1650e-04 - val_rmsle: 0.0245\n",
      "Epoch 7/10\n",
      "41554/41554 [==============================] - 30s 710us/step - loss: 4.4036e-04 - rmsle: 0.0358 - val_loss: 1.6783e-04 - val_rmsle: 0.0200\n",
      "Epoch 8/10\n",
      "41554/41554 [==============================] - 29s 702us/step - loss: 4.2624e-04 - rmsle: 0.0350 - val_loss: 1.8712e-04 - val_rmsle: 0.0219\n",
      "Epoch 9/10\n",
      "41554/41554 [==============================] - 29s 692us/step - loss: 4.3148e-04 - rmsle: 0.0350 - val_loss: 2.1446e-04 - val_rmsle: 0.0237\n",
      "Epoch 10/10\n",
      "41554/41554 [==============================] - 29s 696us/step - loss: 4.2364e-04 - rmsle: 0.0350 - val_loss: 2.4280e-04 - val_rmsle: 0.0265\n",
      "Train on 41554 samples, validate on 10389 samples\n",
      "Epoch 1/10\n",
      "41554/41554 [==============================] - 29s 691us/step - loss: 4.2717e-04 - rmsle: 0.0349 - val_loss: 1.8418e-04 - val_rmsle: 0.0214\n",
      "Epoch 2/10\n",
      "41554/41554 [==============================] - 31s 754us/step - loss: 4.1146e-04 - rmsle: 0.0347 - val_loss: 1.7908e-04 - val_rmsle: 0.0215\n",
      "Epoch 3/10\n",
      "41554/41554 [==============================] - 29s 697us/step - loss: 3.9328e-04 - rmsle: 0.0335 - val_loss: 1.6176e-04 - val_rmsle: 0.0192\n",
      "Epoch 4/10\n",
      "41554/41554 [==============================] - 29s 706us/step - loss: 4.1065e-04 - rmsle: 0.0343 - val_loss: 2.6861e-04 - val_rmsle: 0.0292\n",
      "Epoch 5/10\n",
      "41554/41554 [==============================] - 29s 697us/step - loss: 4.3358e-04 - rmsle: 0.0351 - val_loss: 2.5437e-04 - val_rmsle: 0.0282\n",
      "Epoch 6/10\n",
      "41554/41554 [==============================] - 29s 701us/step - loss: 4.0516e-04 - rmsle: 0.0341 - val_loss: 1.8903e-04 - val_rmsle: 0.0222\n",
      "Epoch 7/10\n",
      "41554/41554 [==============================] - 29s 689us/step - loss: 4.1365e-04 - rmsle: 0.0342 - val_loss: 2.6723e-04 - val_rmsle: 0.0287\n",
      "Epoch 8/10\n",
      "41554/41554 [==============================] - 31s 758us/step - loss: 3.9994e-04 - rmsle: 0.0338 - val_loss: 1.8231e-04 - val_rmsle: 0.0216\n",
      "Epoch 9/10\n",
      "41554/41554 [==============================] - 29s 692us/step - loss: 4.1807e-04 - rmsle: 0.0350 - val_loss: 2.2513e-04 - val_rmsle: 0.0257\n",
      "Epoch 10/10\n",
      "41554/41554 [==============================] - 29s 695us/step - loss: 3.9775e-04 - rmsle: 0.0336 - val_loss: 2.5934e-04 - val_rmsle: 0.0290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.11711674330004748, 0.8757062518261869, 0.007177004873765598)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = Z.copy()\n",
    "g = y.copy()\n",
    "res = []\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "for train_index, test_index in kf.split(M):\n",
    "    M_train, M_test = M.iloc[train_index], M.iloc[test_index]\n",
    "    g_train, g_test = g.iloc[train_index], g.iloc[test_index]\n",
    "    \n",
    "    arr = np.array([])\n",
    "    for i in range(len(M_train)):\n",
    "        row = M_train.iloc[i, :].to_numpy().reshape(-1, 1).reshape(9, 9, 1)\n",
    "        arr = np.append(arr, row)\n",
    "    \n",
    "    arr2 = np.array([])\n",
    "    for i in range(len(M_test)):\n",
    "        row = M_test.iloc[i, :].to_numpy().reshape(-1, 1).reshape(9, 9, 1)\n",
    "        arr2 = np.append(arr2, row)\n",
    "    \n",
    "    M_train_img = arr.reshape(M_train.shape[0], 9, 9, 1)\n",
    "    M_test_img = arr2.reshape(M_test.shape[0], 9, 9, 1)\n",
    "    \n",
    "    model.fit(M_train_img, g_train, batch_size=32, epochs=10, validation_split=0.2)\n",
    "    \n",
    "    M_test_img_fit = M_test_img.copy()\n",
    "    M_test_img_fit[:, 8, 7, 0] = 1 # fitSA\n",
    "    M_test_img_fit[:, 8, 8, 0] = 1 # fitAC\n",
    "    preds = model.predict(M_test_img_fit)\n",
    "    \n",
    "    p = []\n",
    "    for i in range(len(preds)):\n",
    "        p.append(preds[i][0])\n",
    "    \n",
    "    my_df = df.iloc[test_index]\n",
    "    my_df.reset_index(drop=True, inplace=True)\n",
    "    M_test.reset_index(drop=True, inplace=True)\n",
    "    res.append(evaluate(my_df, M_test, data, sent_open_hour_range, p))\n",
    "better = 0\n",
    "equal = 0\n",
    "worst = 0\n",
    "for i in range(5):\n",
    "    better += res[i][0]\n",
    "    equal += res[i][1]\n",
    "    worst += res[i][2]\n",
    "better/5, equal/5, worst/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564baf4a",
   "metadata": {},
   "source": [
    "### Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ebb9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X.copy()\n",
    "X_train, X_test, y_train, y_test, df_train, df_test = split_train_test_by_lifetime(X, df, data, test_size=0.3, random_seed=42)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41999116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(min_samples_leaf=2, n_estimators=200, n_jobs=-1,\n",
       "                      oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42, n_estimators=200, max_features='auto', bootstrap=True, min_samples_split=2, min_samples_leaf=2, oob_score=True, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16d38f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       7.52084304e-11, 9.38162876e-09, 1.09693809e-08, 2.69342123e-08,\n",
       "       8.65996530e-08, 4.25702457e-07, 2.37661734e-06, 2.71065872e-06,\n",
       "       4.06580669e-06, 4.21743583e-06, 4.62722029e-06, 6.05378519e-06,\n",
       "       6.32287580e-06, 6.60320490e-06, 8.05179520e-06, 8.32464737e-06,\n",
       "       9.93244787e-06, 1.17156543e-05, 1.31925514e-05, 1.61741916e-05,\n",
       "       1.79186545e-05, 2.17040792e-05, 2.20587716e-05, 2.29594757e-05,\n",
       "       2.33489365e-05, 2.49766481e-05, 2.59708891e-05, 2.61757940e-05,\n",
       "       2.63634537e-05, 2.64203060e-05, 2.64454619e-05, 2.65590814e-05,\n",
       "       2.76705290e-05, 2.84390965e-05, 2.88548401e-05, 2.90051257e-05,\n",
       "       2.92254031e-05, 3.33811974e-05, 3.34976622e-05, 3.43636092e-05,\n",
       "       3.46404061e-05, 3.65889321e-05, 3.84824966e-05, 3.86909509e-05,\n",
       "       4.53776871e-05, 4.60149568e-05, 4.72311998e-05, 4.91061249e-05,\n",
       "       5.12355836e-05, 5.18259897e-05, 5.39634992e-05, 6.39551095e-05,\n",
       "       6.85616918e-05, 7.34089380e-05, 7.52721841e-05, 9.07632959e-05,\n",
       "       9.79761779e-05, 1.01424544e-04, 1.03085486e-04, 1.15432564e-04,\n",
       "       1.16464750e-04, 1.16682019e-04, 1.39507900e-04, 1.52705238e-04,\n",
       "       1.68907181e-04, 1.76741406e-04, 2.17729067e-04, 2.34324948e-04,\n",
       "       2.43884333e-04, 2.58887465e-04, 2.77808219e-04, 3.00907883e-04,\n",
       "       3.02627968e-04, 3.22633107e-04, 3.56126085e-04, 3.59225051e-04,\n",
       "       4.93122520e-04, 4.96692061e-04, 5.18819612e-04, 5.20178227e-04,\n",
       "       5.77278661e-04, 5.94180545e-04, 6.26673868e-04, 6.68500721e-04,\n",
       "       8.42236258e-04, 8.59883531e-04, 1.07409281e-03, 4.56127640e-03,\n",
       "       5.60438882e-03, 7.22541167e-03, 7.97621409e-03, 1.37951346e-02,\n",
       "       8.41197259e-02, 8.63777722e-01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "956ac201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Z.columns[np.where(rf.feature_importances_ > 6.60320490e-06)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33f90d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.drop(Z.columns.difference(Z.columns[np.where(rf.feature_importances_ > 6.60320490e-06)]), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ea71ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "Z_train, Z_test, y_train, y_test, df_train, df_test = split_train_test_by_lifetime(Z, df, data, test_size=0.34, random_seed=42)\n",
    "Z_train.reset_index(drop=True, inplace=True)\n",
    "Z_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a72f0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([])\n",
    "for i in range(len(Z_train)):\n",
    "    row = Z_train.iloc[i, :].to_numpy().reshape(-1, 1).reshape(9, 9, 1)\n",
    "    arr = np.append(arr, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6da1ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = np.array([])\n",
    "for i in range(len(Z_test)):\n",
    "    row = Z_test.iloc[i, :].to_numpy().reshape(-1, 1).reshape(9, 9, 1)\n",
    "    arr2 = np.append(arr2, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a519d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train_img = arr.reshape(Z_train.shape[0], 9, 9, 1)\n",
    "Z_test_img = arr2.reshape(Z_test.shape[0], 9, 9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0043702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34282 samples, validate on 8571 samples\n",
      "Epoch 1/10\n",
      "34282/34282 [==============================] - 27s 800us/step - loss: 0.0768 - rmsle: nan - val_loss: 0.0026 - val_rmsle: 0.0962\n",
      "Epoch 2/10\n",
      "34282/34282 [==============================] - 23s 682us/step - loss: 0.0065 - rmsle: nan - val_loss: 0.0024 - val_rmsle: 0.0929\n",
      "Epoch 3/10\n",
      "34282/34282 [==============================] - 26s 768us/step - loss: 0.0042 - rmsle: 0.1226 - val_loss: 0.0024 - val_rmsle: 0.0895\n",
      "Epoch 4/10\n",
      "34282/34282 [==============================] - 24s 697us/step - loss: 0.0031 - rmsle: 0.1057 - val_loss: 0.0011 - val_rmsle: 0.0641\n",
      "Epoch 5/10\n",
      "34282/34282 [==============================] - 24s 696us/step - loss: 0.0023 - rmsle: 0.0905 - val_loss: 0.0010 - val_rmsle: 0.0603\n",
      "Epoch 6/10\n",
      "34282/34282 [==============================] - 24s 687us/step - loss: 0.0018 - rmsle: 0.0800 - val_loss: 6.3551e-04 - val_rmsle: 0.0481\n",
      "Epoch 7/10\n",
      "34282/34282 [==============================] - 24s 687us/step - loss: 0.0015 - rmsle: 0.0736 - val_loss: 3.7304e-04 - val_rmsle: 0.0385\n",
      "Epoch 8/10\n",
      "34282/34282 [==============================] - 24s 695us/step - loss: 0.0013 - rmsle: 0.0672 - val_loss: 0.0015 - val_rmsle: 0.0697\n",
      "Epoch 9/10\n",
      "34282/34282 [==============================] - 24s 688us/step - loss: 0.0012 - rmsle: 0.0631 - val_loss: 5.7915e-04 - val_rmsle: 0.0441\n",
      "Epoch 10/10\n",
      "34282/34282 [==============================] - 24s 687us/step - loss: 0.0010 - rmsle: 0.0588 - val_loss: 7.3811e-04 - val_rmsle: 0.0534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f50ec0f9400>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Z_train_img, y_train, batch_size=32, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bed9fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_test_img_fit = Z_test_img.copy()\n",
    "Z_test_img_fit[:, 8, 7, 0] = 1 # fitSA\n",
    "Z_test_img_fit[:, 8, 8, 0] = 1 # fitAC\n",
    "preds = model.predict(Z_test_img_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d51a3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, df_train, df_test = split_train_test_by_lifetime(X, df, data, test_size=0.34, random_seed=42)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04fb0d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16385050962627407, 0.832797281993205, 0.0033522083805209513)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = []\n",
    "for i in range(len(preds)):\n",
    "    p.append(preds[i][0])\n",
    "evaluate(df_test, X_test, data, sent_open_hour_range, p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
